%% LyX 2.4.3 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}
\usepackage{textcomp}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{url}
\usepackage{varwidth}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{Constructing artificial features with Grammatical Evolution for earthquake
prediction}

\TitleCitation{Constructing artificial features with Grammatical Evolution for earthquake
prediction}

\Author{Constantina Kopitsa$^{1}$, Glykeria Kyrou$^{2}$\textsuperscript{},
Vasileios Charilogis$^{3}$ and Ioannis G. Tsoulos$^{4,*}$}

\AuthorNames{Kopitsa, C., Kyrou, G., Charilogis V. \textbackslash\& Tsoulos,
I.G. }

\AuthorNames{Constantina Kopitsa, Glykeria Kyrou, Vasileios Charilogis and Ioannis
G. Tsoulos}

\AuthorCitation{Kopitsa, C.; Kyrou, G.; Charilogis, V., Tsoulos, I.G. }


\address{$^{1}$\quad{}Department of Informatics and Telecommunications.
University of Ioannina, Greece k.kopitsa@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications. University
of Ioannina, Greece g.kyrou@uoi.gr\\
$^{3}\quad$Department of Informatics and Telecommunications. University
of Ioannina, Greece v.charilog@uoi.gr\\
$^{4}\quad$Department of Informatics and Telecommunications. University
of Ioannina, Greece itsoulos@uoi.gr}


\corres{Correspondence: itsoulos@uoi.gr}


\abstract{Earthquakes are the result of the dynamic processes occurring beneath
the Earth’s crust specifically, the movement and interaction of tectonic
/ lithospheric plates. When one plate shifts relative to another,
stress accumulates and is eventually released as seismic energy. This
process is continuous and unstoppable. This phenomenon is well recognized
in the Mediterranean region, where significant seismic activity arises
from the northward convergence (4-10 mm per year) of the African plate
relative to the Eurasian plate along a complex plate boundary. Consequently,
our research will focus on the Mediterranean region, specifically
examining seismic activity from 1990 - 2015 within the latitude range
of 33-44° and longitude range of 17-44°. These geographical coordinates
encompass 28 seismic zones, with the most active areas being Turkey
and Greece. In this paper we applied Grammatical Evolution for artificial
feature construction in earthquake prediction, evaluated against machine
learning approaches including MLP(GEN), MLP(PSO), SVM, and NNC. Experiments
showed that Feature Construction (FC) achieved the best performance,
with a mean error of 9.05\% and overall accuracy of 91\%, outperforming
SVM. Further analysis revealed that a single constructed feature $\left(N_{f}=1\right)$
yielded the lowest average error (8.21\%), while varying the number
of generations indicated that $N_{g}=200$ provided an effective balance
between computational cost and predictive accuracy. These findings
confirm the efficiency of FC in enhancing earthquake prediction models
through artificial feature construction. Our results, as will be discussed
in greater detail within the research, yield an average error of approximately
9\%, corresponding to an overall accuracy of 91\%.}


\keyword{Earthquakes; Machine learning; Neural networks; Grammatical Evolution;
Feature Construction}

\DeclareTextSymbolDefault{\textquotedbl}{T1}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
%% Variable width box for table cells
\newenvironment{cellvarwidth}[1][t]
    {\begin{varwidth}[#1]{\linewidth}}
    {\@finalstrut\@arstrutbox\end{varwidth}}
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxcode}
	{\par\begin{list}{}{
		\setlength{\rightmargin}{\leftmargin}
		\setlength{\listparindent}{0pt}% needed for AMS classes
		\raggedright
		\setlength{\itemsep}{0pt}
		\setlength{\parsep}{0pt}
		\normalfont\ttfamily}%
	 \item[]}
	{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aichem, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\setcounter{page}{\@firstpage}
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{}
\daterevised{ } % Comment out if no revised date
\dateaccepted{}
\datepublished{}
%\datecorrected{} % For corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers include a "RETRACTED: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Different journals have different requirements. Please check the specific journal guidelines in the "Instructions for Authors" on the journal's official website.
 
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent The goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2 bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\maketitle

\section{Introduction}

When entering the single keyword “earthquakes” into Google Scholar,
more than 3,910,000 results are retrieved, demonstrating the intense
interest that exists in the field of seismology. Thanks to these studies
and investigations, which evolve from an initial idea or theory into
practical applications, it can be stated with confidence that humanity
is now capable of achieving timely early warning before seismic events
occur. Consequently, the pursuit of sustainability strengthens our
resilience against seismic phenomena. This has been achieved through
the implementation of early warning systems established across the
globe, particularly in technologically advanced countries that are
also seismically vulnerable regions. For this purpose, the UN Disaster
Risk Reduction was established, which is “aimed at preventing new
and reducing existing disaster risk and managing residual risk, all
of which contribute to strengthening resilience and therefore to the
achievement of sustainable development ”\citep{UNDRR 2017}. An early
achievement of Disaster Risk Reduction took place in Japan in 1960,
when seismic sensors were installed along the railway infrastructure
to ensure the automatic immobilization of trains \citep{Nakamura 1984}.
The Japanese UrEDAS (Urgent Earthquake Detection and Alarm System)
has been described as the ``grandfather'' of earthquake early warning
systems in general, and of onsite warning systems in particular \citep{Nakamura 1988}.
Since then, techniques and methods have advanced through technological
progress. The next achievement in early earthquake warning was accomplished
in Mexico in 1989 with the establishment of the Seismic Alert System
(SAS) \citep{Espinosa Aranda 1995}. In 2006, Japan launched the Earthquake
Early Warning system initially for a limited audience and subsequently
for the general public, in order to ensure the effectiveness of EEW
in disaster mitigation \citep{Kamigaichi 2009}. This allows an earthquake
warning to be disseminated between several seconds and up to one minute
prior to the occurrence of the event \citep{EEW}. In Bucharest, Romania,
an earthquake warning system, in 1999, was also developed, providing
a preparation window of 25 seconds \citep{Wenzel 1999}. Also in Istanbul,
in preparation for the anticipated earthquake, an early warning system
was implemented in 2002 \citep{Alcik 2009}. In Southern Europe, at
the University of Naples in Italy, a software model called PRESTo
(PRobabilistic and Evolutionary early warning SysTem) was developed,
designed to estimates of earthquake location and size within 5-6 seconds
\citep{Satriano 2009}. Furthermore, the United States, through the
U.S. Geological Survey, has established its own earthquake warning
system. Since 2016, the ShakeAlert system has been operational along
the West Coast \citep{USGS,Shake alert}. Subsequently, a map is presented
in Figure \ref{fig:mapQuakes}, illustrating Earthquake Early Warning
Systems worldwide, with colors indicating the operational status of
each system, provided in \citep{zollo}. Purple denotes operative
systems that provide warnings to the public, black represents systems
currently undergoing real-time testing, and gray is used for countries
where feasibility studies are still in progress. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.65]{table1}
\par\end{centering}
\caption{The map shows Earthquakes Early Warning Systems around the World\label{fig:mapQuakes}}
\end{figure}

Within this framework, it is important to highlight that in recent
years considerable emphasis has been placed on the advancement of
diverse models for the early detection of seismic events, which have
become available to the wider public through mobile applications,
television broadcasts, and radio communication \citep{Tsoulos 2025}.
The primary function of Android applications is that, once an earthquake
is detected, an alert is transmitted to all smartphones located within
the affected area. Provided that the user is not in close proximity
to the epicenter, the notification can be received in advance, allowing
sufficient time to take protective action before the destructive seismic
waves arrive \citep{Earthquake app,Android EAS,Greece Earthquakes,Earthquake Network}.
By harnessing technology as an ally against natural disasters, humanity
can move beyond the devastating consequences of major earthquakes,
such as the 2004 Indian Ocean event with more than 220,000 fatalities,
the 2011 Tōhoku earthquake in Japan with over 19,000 losses, and the
2023 Turkey-Syria earthquake with more than 43,000 deaths. Accordingly,
resilience and sustainability for populations affected by seismic
events encompass both physical and social infrastructures capable
of withstanding earthquakes, while simultaneously safeguarding long-term
well-being through disaster risk reduction, community preparedness,
and equitable recovery. 

Subsequently, we will proceed with the presentation of related studies
alongside our own, which progressively enhance both our sustainability
and our capacity for prevention against seismic phenomena.\textbf{
}Housner, in 1964, concluded that artificial earthquakes constitute
adequate representations of strong-motion events for structural analysis
and may serve as standard ground motions in the design of engineering
structures \citep{Housner 1964}.\textbf{ }Adeli, in 2009, proposed
a novel feature extraction technique, asserting that when combined
with a selected Probabilistic Neural Network (PNN), it can yield reliable
prediction outcomes for earthquakes with magnitudes ranging from 4.5
to 6.0 on the Richter scale \citep{Adeli 2009}. Zhou, in 2012, introduced
a robust feature extraction approach, the Log-Sigmoid Frequency Cepstral
Coefficients (LSFCC), derived from the Mel Frequency Cepstral Coefficients
(MFCC), for the classification of ground targets using geophones.
Employing LSFCCs, the average classification accuracy for tracked
and wheeled vehicles exceeds 89\% across three distinct geographical
settings, achieved with a single classifier trained in only one of
these environments \citep{Zhou 2012}. Martinez-Alvarez, in 2013,
investigated the utilization of various seismicity indicators as inputs
for artificial neural networks. The study proposes combining multiple
indicators-previously shown to be effective across different seismic
regions through the application of feature selection techniques \citep{Mart=0000EDnez-=0000C1lvarez 2013}.
Schmidt, in 2015, proposed an efficient and automated method for seismic
feature extraction. The central concept of this approach is to interpret
a two-dimensional seismic image as a function defined on the vertices
of a carefully constructed underlying graph \citep{Schmidt 2015}.
Narayanakumar, in 2016, extracted seismic features from a predetermined
number of events preceding the main shock in order to perform earthquake
prediction using the Backpropagation (BP) neural network technique
\citep{Narayanakumar 2016}. Cortes, in 2016, sought to identify the
parameters most effective for earthquake prediction. As various studies
have employed different feature sets, the optimal selection of features
appears to depend on the specific dataset used in constructing the
model \citep{Cortes 2016}. Asim, in 2018, developed a hybrid embedded
feature selection approach designed to enhance the accuracy of earthquake
prediction \citep{Asim 2018}. Chamberlain, in 2018, demonstrated
that synthetic seismograms, when applied with matched-filter techniques,
enable the detection of earthquakes even with limited prior knowledge
of the source \citep{Chamberlain 2018}. Okada, in 2018, employed
observational data either by calibrating parameters within existing
models or by deriving models and indicators directly from the data
itself \citep{Okada 2018}. Lin, in 2018, employed the earthquake
catalogue from 2000 to 2010, comprising events with a Richter magnitude
(ML) of 5 and a depth of 300 km within the study area (21°-26° N,
119°-123° E). This dataset was utilized as training input to develop
the initial earthquake magnitude prediction backpropagation neural
network (IEMPBPNN) model, which was designed with two hidden layers
\citep{Lin 2018}. Zhang, in 2019, proposed a precursory pattern-based
feature extraction approach aimed at improving earthquake prediction
performance. In this method, raw seismic data are initially segmented
into fixed daily time intervals, with the magnitude of the largest
earthquake within each interval designated as the main shock \citep{Zhang 2019}.
Rohas's, in 2019, paper reviewed the latest uses of artificial neural
networks for automated seismic-data interpretation, focusing especially
on earthquake detection and onset-time estimation \citep{Rojas 2019}.\textbf{
}Ali, in 2020, generated synthetic seismic data for a three-layer
geological model and analyzed using Continuous Wavelet Transform (CWT)
to identify seismic reflections in both the temporal and spatial domains
\citep{Ali 2020}. Bamer, in 2021, demonstrated through comparison
with several state-of-the-art studies, that the convolutional neural
network autonomously learns to extract the pertinent input features
and structural response behavior directly from complete time histories,
rather than relying on a predefined set of manually selected intensity
measures \citep{Bamer 2021}. Wang, in 2023, reports that the accuracies
of various AI models using the feature extraction dataset surpassed
those obtained with the spectral amplitude dataset, demonstrating
that the feature extraction approach more effectively emphasizes the
distinctions among different types of seismic events \citep{Wang 2023}.
Ozkaya, in 2024, developed a novel feature engineering framework that
integrates the Butterfly Pattern (BFPat), statistical measures, and
wavelet packet decomposition (WPD) functions. The proposed model achieved
an accuracy of 99.58\% in earthquake detection and 93.13\% in three-class
wave classification \citep{Ozkaya 2024}.\textbf{ }Sinha’s review,
in 2025, offers valuable insights into cutting-edge techniques and
emerging directions in feature engineering for seismic prediction,
highlighting the importance of interdisciplinary collaboration in
advancing earthquake forecasting and reducing seismic risk \citep{Sinha 2025}.
Mahmoud, in 2025, investigates the application of machine learning
approaches to earthquake classification and prediction using synthetic
seismic datasets \citep{Mahmoud 2025}.

In contrast to the aforementioned studies, this paper introduces a
novel approach constructing artificial features with Grammatical Evolution
\citep{mainGe} for earthquake prediction, marking the first application
of this evolutionary technique in the field of seismology. While the
method does not directly address a specific gap in the literature,
the experiments demonstrated that feature construction achieved an
overall accuracy of 91\%, thereby contributing to the advancement
of earthquake prediction research. We consider this contribution significant
in enhancing the understanding of seismic phenomena and in highlighting
the potential of Grammatical Evolution as a promising tool for feature
engineering in geophysical datasets. In particular, the method of
Grammatical Evolution can be considered as a genetic algorithm \citep{gaOverview}
with integer chromosomes. Each chromosome contains a series of production
rules from the provided Backus-Naur form (BNF) grammar \citep{bnf1}
of the underlying language and hence this method can create programs
that belong to this language. This procedure have been used with success
in various cases, such as\textbf{ }data fitting problems \citep{ge_program1,ge_program2},\textbf{
}problems that appear in economics \citep{ge_credit},\textbf{ }computer
security problems \citep{ge_intrusion}, problems related to water
quality \citep{ge_water}, problems appeared in medicine \citep{ge_glykemia},\textbf{
}evolutionary computation \citep{ge_ant},\textbf{ }hardware issues
in data centers \citep{ge_datacenter},\textbf{ }solution of trigonometric
problems \citep{ge_trig},\textbf{ }automatic composition of music
\citep{ge_music}, dynamic construction of neural networks \citep{ge_nn,ge_nn2},
automatic construction of constant numbers \citep{ge_constant}, playing
video games \citep{ge_pacman,ge_supermario}, problems regarding energy
\citep{ge_energy}, combinatorial optimization \citep{ge_comb}, security
issues \citep{ge_crypt}, automatic construction of decision trees
\citep{ge_decision}, problems in in electronics \citep{ge_analog},
automatic construction of bounds for neural networks \citep{Tsoulos 2023},
construction of Radial Basis Function networks \citep{Tsoulos 2024}
etc. This research work focuses on the creation of artificial features
from existing ones, aiming at two goals: on the one hand, it seeks
to reduce the required information required for the correct classification
of seismic data and on the other hand, it seeks to highlight the hidden
nonlinear correlations that may exist between the existing features
of the objective problem. In this way, a significant improvement in
the classification of seismic data will be achieved.

Beyond the Grammatical Evolution approach proposed in this study,
which achieved an accuracy of 91\%, we will also provide a concise
comparison with results from related research that has employed alternative
machine learning techniques in the field of seismology. In the study
by Adeli et al. \citep{Adeli 2009} the PNN model demonstrated satisfactory
predictive accuracy for earthquakes with magnitudes between 4.5 and
6.0, but its performance declined considerably for events exceeding
magnitude 6.0.\textbf{ }Zhou et al. \citep{Zhou 2012} reported that
the application of the Log-Sigmoid Frequency Cepstral Coefficients
method achieved an average classification accuracy exceeding 89\%.
Narayanakumar et al.\citep{Narayanakumar 2016} investigated the prediction
of moderate earthquakes (magnitude 3.0-5.8). While the seismometer
recorded an event of magnitude 4.0, the BP-ANN model predicted magnitudes
in the range of 3.0-5.0, achieving a success rate between 75\% and
125\%. According to Asim et al., \citep{Asim 2018} the SVR-HNN prediction
model achieved its highest performance in Southern California, with
MCC, R score, and accuracy values of 0.722, 0.623, and 90.6\%, respectively.
The Chilean region ranked second, yielding an MCC of 0.613, an R score
of 0.603, and an accuracy of 84.9\%. In contrast, the Hindukush region
exhibited the lowest performance, with MCC, R score, and accuracy
values of 0.600, 0.580, and 82.7\%, respectively. Chamberlain et al.
\citep{Chamberlain 2018} reported that template-based detections
produced 7,340 events, of which 3,577 were identified as duplicates,
whereas the STA/LTA method yielded 682 detections over the same time
interval. Lin et al. \citep{Lin 2018} reported that the average magnitude
error in Taiwan was $\Delta ML=\pm0.3$, while the training errors
of the EEMPBPNN model (\textless 0.25) remained below this average
error threshold. Zhang et al. \citep{Zhang 2019} demonstrated that
the proposed method achieved prediction accuracies of 93.26\% and
92.07\% across the two datasets examined. Rojas et al. \citep{Rojas 2019}
reported recognition performance of approximately 99.2\% for P-wave
signals and 98.4\% for pure noise. Ali et al. \citep{Ali 2020} conducted
a statistical analysis using a 95\% confidence interval for the normalized
CWT coefficients of P-wave velocity, seismic trace, acoustic impedance,
and synthetic seismic trace. Wang et al. \citep{Wang 2023} found
that, in the model generalization evaluation, the two-class models
trained on the 36-dimensional network-averaged dataset achieved test
accuracies and F1-scores exceeding 90\%. Ozkaya et al. \citep{Ozkaya 2024}
reported that their model achieved an accuracy of 99.58\% in earthquake
detection and 93.13\% in three-class wave classification. While comparable
studies have demonstrated strong performance, many of them are limited
to specific regions of interest or focus on particular signal types
such as acoustic wave detection.\textbf{ }In contrast, the Grammatical
Evolution approach presented here not only achieves competitive accuracy
(91\%) but also demonstrates broader applicability across diverse
seismic datasets, thereby highlighting its potential as a more generalizable
solution in earthquake event discrimination.

The rest of this manuscript is organized as follows: the used dataset
and the incorporated methods used in the conducted experiments are
outlined in section \ref{sec:Materials-and-Methods}, the experimental
results are shown and discussed in section \ref{sec:Results} and
finally a detailed discussion is provided in section \ref{sec:Conclusions}.

\section{Materials and Methods\label{sec:Materials-and-Methods}}

In this section, a detailed presentation of the datasets used as well
as the machine learning techniques used in the experiments performed
will be provided.

\subsection{The Dataset Employed}

In this study, we made use of open data from the NSF Seismological
Facility for the Earth Consortium (SAGE) available from \url{https://ds.iris.edu/}(accessed
on 25 November 2025), which is a platform offering an interactive
global map that facilitates both data visualization and the real-time
extraction of datasets from the displayed geographic regions. The
area defined by latitudes 33°-44° and longitudes 17°-44° covers 28
seismic zones, with Turkey and Greece identified as the most seismically
active regions. 

Regarding the 28 seismic zones listed in Table \ref{tab:Regions-codes-for},
these correspond to specific geographic coordinates (latitudes and
longitudes) provided through the interactive seismic platform previously
referenced. The platform grants direct access to these spatial data,
ensuring that the delineation of the zones is both systematic and
consistent with the established geospatial framework. 

\begin{table}[H]
\caption{Regions codes for the 28 seismic zones.\label{tab:Regions-codes-for}}

\centering{}%
\begin{tabular}{cc}
\hline 
Region & Region Code\tabularnewline
\hline 
Turkey & 1\tabularnewline
Crete Greece & 2\tabularnewline
Greece & 3\tabularnewline
Dodecanese Islands, GREECE & 4\tabularnewline
Ionian Sea & 5\tabularnewline
Aegean Sea & 6\tabularnewline
Southern Greece & 7\tabularnewline
GREECEALBANIA border region & 8\tabularnewline
GREECEBULGARIA border region & 9\tabularnewline
Cyprus region & 10\tabularnewline
Albania & 11\tabularnewline
NORTHWESTERN Balkan Region & 12\tabularnewline
Central Mediterranean sea & 13\tabularnewline
Jordan Syria region & 14\tabularnewline
\begin{cellvarwidth}[t]
\centering
GEORGIA ARMENIA TURKEY \\
Border Region
\end{cellvarwidth} & 15\tabularnewline
TURKEY IRAN border region & 16\tabularnewline
Iraq & 17\tabularnewline
Eastern Mediterranean sea & 18\tabularnewline
NORTHWESTERN CAUCASUS & 19\tabularnewline
Black Sea & 20\tabularnewline
Romania & 21\tabularnewline
Bulgaria & 22\tabularnewline
Crimea Region Ukraine & 23\tabularnewline
\begin{cellvarwidth}[t]
\centering
ARMENIA AZERBAIJAN IRAN \\
border region
\end{cellvarwidth} & 24\tabularnewline
Adriatic sea & 25\tabularnewline
\begin{cellvarwidth}[t]
\centering
UKRAINE MOLDOVA RUSSIA \\
border region
\end{cellvarwidth} & 26\tabularnewline
Southern Italy & 27\tabularnewline
IRAN IRAQ border region & 28\tabularnewline
\hline 
\end{tabular}
\end{table}

Furthermore, the selection of NSF data was driven by its advanced
functionality and broad accessibility. Notably, it supports the download
of up to 25,000 records per file, thereby streamlining the workflow
and improving the efficiency of information retrieval. The region
under investigation is presented in Figure \ref{fig:The-study-area.}.
This image was retrieved from IRIS Earthquake Browser, in the following
URL: \url{https://ds.iris.edu/ieb/index.html?format=text&nodata=404&starttime=2010-01-01&endtime=2010-12-31&orderby=time-desc&src=iris&limit=1000&maxlat=74.85&minlat=-7.26&maxlon=135.19&minlon=-97.36&zm=3&mt=ter}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.65]{table2}
\par\end{centering}
\caption{The study area.\label{fig:The-study-area.}}

\end{figure}


\subsection{Dataset Description}

We obtained and systematically analyzed 511,064 earthquake events
from 1990 to 2015, as this time period is of particular scientific
interest due to the surge in seismic activity, as also illustrated
in the graph of Figure \ref{fig:Graphs-seismic-events}. Specifically,
we selected this time period because it encompasses a wide range of
earthquake magnitudes, which provides a diverse dataset conducive
to algorithm training and supports the construction of artificial
features via Grammatical Evolution. 

In the years under examination, earthquakes are distributed across
all magnitude classes, exhibiting an almost ideal proportion consistent
with the Gutenberg-Richter law \citep{gutenberg}. For instance, within
the first two five-year periods of our dataset, 2 events above magnitude
8.0, 3 events above magnitude 7.0, 19 events above magnitude 6.0,
165 events between 5.0-5.9, 3,048 events between 4.0-4.9, and 26,301
events between 3.0-3.9 were recorded, thereby confirming the expected
logarithmic relationship between frequency and magnitude. Such a balanced
and representative dataset further enhances the effectiveness of the
Grammatical Evolution approach, as it provides a comprehensive distribution
of seismic events across the full spectrum of magnitudes. Moreover,
our analysis led us to the conclusion that the datasets from 1970-1989
and 2016-2025 lack the diversity observed in the dataset we selected.
Specifically, classes 6 and 7 are entirely absent from the 1970-1989
records, while in the post-2016 data class 7 is missing and class
1 contains only a limited number of instances, namely 25.

\begin{figure}[H]
\caption{Graphs seismic events from 1970 - 2025 (Area Study)\label{fig:Graphs-seismic-events}}

$ $$ $

$ $
\begin{centering}
\includegraphics{\string"table 3.graphic1\string".jpg}
\par\end{centering}
\end{figure}

On the platform that provides us with the Interactive Earthquake Browser
we employed coordinates spanning latitudes 33°-44° and longitudes
17°-44°, considered magnitude values from 1.0 to 10.0, and incorporated
all available depths by default within the depth range. The raw dataset
included the following variables: year, month, day, time, latitude
(Lat), longitude (Lon), depth, magnitude, region, and timestamp. Accordingly,
Table \ref{tab:Raw-Data-from} provides a detailed overview of the
raw dataset.

\begin{table}
\centering
\caption{Raw Data from NSF Interactive Earthquake Browser (1990 - 2015)\label{tab:Raw-Data-from}}

\begin{tabular*}{10cm}{@{\extracolsep{\fill}}cc}
\hline 
\textbf{Raw Data} & \tabularnewline
\hline 
\textbf{Features} & \textbf{Range}\tabularnewline
Year & 1990 - 2015\tabularnewline
Month & 1 - 12\tabularnewline
Day & 1- 31\tabularnewline
Time & 00:00:00 - 23:59:59\tabularnewline
Latitude & 33 - 44\tabularnewline
Longitude & 17 - 44\tabularnewline
Region & 1 - 28\tabularnewline
Depth & 0.00 - 800.00\tabularnewline
Magnitude & 1 - 10\tabularnewline
Timestamp & \tabularnewline
\hline 
\end{tabular*}
\end{table}

Subsequently, a preprocessing procedure was applied to the dataset.
This included the identification of the lithospheric plate associated
with each earthquake, to which a unique code was assigned. Furthermore,
the months were categorized according to the four seasons, the days
were grouped into ten-day intervals, and the time of occurrence was
classified into four periods (morning, noon, afternoon, and night).
The focal depth was divided into six categories. In addition, a new
column was created to indicate, with a binary value (0 or 1), whether
an earthquake had previously occurred in the same region during the
same season. Finally, the dataset was merged with the Kp index, representing
geomagnetic storm activity, which was further classified into six
distinct categories. The final dataset was further processed, including
the following: Year, Epoch Code, Day Code, Time Code, Latitude, Longitude,
Depth Code, Previous Magnitude Code, Same Region Code, Lithospheric/Tectonic
Plate, Kp Code. This information is outlined in Table \ref{tab:Utilized-Data-from}.

\begin{table}
\centering
\caption{Utilized Data from NSF Interactive Earthquake Browser (1990 - 2015)\label{tab:Utilized-Data-from}}

\begin{tabular*}{10cm}{@{\extracolsep{\fill}}ccc}
\hline 
\textbf{Utilized Data} &  & \tabularnewline
\hline 
\textbf{Features} & \textbf{Range} & \textbf{Class}\tabularnewline
Year & 1990 - 2015 & \tabularnewline
Epoch Code & 1 - 12 & 0 -3\tabularnewline
Day Code & 1 - 31 & 0 - 2\tabularnewline
Time Code & 00:00:00 - 23:59:59 & 0 - 3\tabularnewline
Latitude & 33 - 44 & \tabularnewline
Longitude & 17 - 44 & \tabularnewline
Depth Code & 0.00 - 800.00 & 0 - 5\tabularnewline
Previous Magnitude Code & 1 - 10 & 1 - 7\tabularnewline
Same Region Code & 1 - 28 & 1 - 28\tabularnewline
Lithospheric Code & 1 - 7 & 1 - 7\tabularnewline
Kp Code & 0.000 - 9.000 & 0 - 5\tabularnewline
\hline 
\end{tabular*}
\end{table}
In the processed dataset, categorical classes were introduced to facilitate
the analysis. Specifically, the months were initially ordered numerically
and subsequently grouped according to the corresponding season. Thus,
class 0 represents the winter months, class 1 corresponds to the spring
months, class 2 to the summer months, and class 3 to the autumn months.
The days comprising each month were divided into three ten-day intervals,
with the first interval assigned to class 0, the second interval to
class 1, and the final interval to class 2. Accordingly, the hours
of the 24 hour day were divided into distinct periods, with the morning
zone assigned to class 0, midday to class 1, afternoon to class 2,
and night to class 3. For the depth code, earthquakes occurring near
the surface (0-32.9 km) were assigned to class 0, those recorded at
depths between 33-69.9 km to class 1, events within 70-149.9 km to
class 2, those between 150-299.9 km to class 3, earthquakes at depths
of 300-499.9 km to class 4, those between 500-799.9 km to class 5,
and finally, events occurring at depths greater than 800 km were assigned
to class 6. Continuing, earthquakes with magnitudes below 2.9 were
assigned to class 1, those with magnitudes between 3.0-3.9 to class
2, magnitudes between 4.0-4.9 to class 3, magnitudes between 5.0--5.9
to class 4, magnitudes between 6.0--6.9 to class 5, magnitudes between
7.0-7.9 to class 6, while events with magnitudes of 8.0 and above
were assigned to class 7. With regard to the categorization of the
geographical region and the lithospheric plate, each region was assigned
a code ranging from 1 to 28. Similarly, a code from 1 to 7 was allocated
to the lithospheric plates corresponding to each seismic event. Finally,
the Kp code was classified as follows: values ranging from 0.000-4.000
were assigned to class 0, those from 4.100--5.000 to class 1, values
between 5.100--6.000 to class 2, those from 6.100--7.000 to class
3, values between 7.100-8.000 to class 4, and finally, values from
8.100-9.000 were assigned to class 5.

At the following stage of data processing, we elected to focus on
earthquakes with a magnitude code of 2 and above, since the inclusion
of lower-magnitude events would bias the model toward predicting minor
seismic occurrences. Notably, the small-magnitude category (1-2.9
mag) prevailed as the majority class with 407,144 records, creating
a significant imbalance in the dataset. By excluding this dominant
class, we aimed to achieve a more balanced distribution across magnitudes
and to enhance the representational capacity of the model. This approach
is consistent with previous studies that have similarly excluded small
earthquakes to mitigate bias and improve predictive performance. Wang
et al., in 2023, reports in Dataset and Feature Engineering “ The
seismic catalog used in this study was obtained from the China Earthquake
Data Center (CEDC, \url{http://data.earthquake.cn/}, last accessed
on 12 December 2025) and includes earthquake events with a magnitude
greater than 3.0 in the Sichuan-Yunnan region from 1970 to 2021” \citep{Wang}.
It is noteworthy that the division into two classes within the field
of seismology has previously been employed, as in the study of Zhang
et al. (2023), which investigated seismic data by categorizing events
into high magnitude ($M\ge5.5$) and low magnitude ($M<5.5$) \citep{zhu_rapid}.\textbf{
}Following these steps, we proceeded with our experiments, utilizing
approximately 10,000 seismic events in order to generate artificial
features through Grammatical Evolution.

In summary, the seismic data employed in this study were recorded
on the magnitude scale, covering events from 1 to 10. Initially, we
excluded small earthquakes, which predominated in number and affected
the balance of the dataset. The data source was the EarthScope Consortium,
which operates the NSF Geodetic Facility for the Advancement of Geoscience
(GAGE) and the NSF Seismological Facility for the Advancement of Geoscience
(SAGE). These records were obtained from thousands of stations worldwide,
constituting the primary NSF SAGE archive.

\subsection{Global optimization methods for neural network training\label{subsec:Global-optimization-methods}}

In the current work two well - known global optimization methods were
incorporated for neural network training \citep{nn1,nn2}, the Genetic
Algorithm and the Particle Swarm Optimization (PSO) method. The Genetic
algorithms is an evolutionary optimization method designed to minimize
an objective function defined over a continuous search space. It operates
on a population of candidate solutions, each represented as a vector
of parameters and this population is evolved through a series of steps,
where in each step some process that resemble natural processes are
applied to the population. Genetic algorithms have applied on a wide
range applications, that include training of neural networks \citep{nnga1,nnga2}.
Beyond neural network training, Genetic Algorithms have demonstrated
strong performance in physics, biotechnology, and medical physics.
In bioinformatics and biotechnology, they are commonly applied to
the analysis of biological data, gene selection, and the optimization
of complex biological systems \citep{genapp1}. In medical physics
and biomedical engineering, GAs have been successfully used for medical
image segmentation \citep{genapp2}, diagnostic modeling, and the
optimization of treatment parameters \citep{genapp3}. They have also
been applied to multi-objective problems related to energy efficiency,
economic viability, environmental life-cycle assessment \citep{genapp4}
and parameter identification in complex multi-physics energy systems
\citep{genapp5}. Also, Figure \ref{fig:GASteps} presents the basic
steps of a genetic algorithm.

\begin{figure}[H]
\includegraphics[scale=0.75]{ga_schema}

\caption{The main steps of the Genetic algorithm.\label{fig:GASteps}}
\end{figure}

Particle Swarm Optimization (PSO) is a population-based search method
inspired by how animals move and cooperate in groups like flocks of
birds or schools of fish \citep{pso1,pso2}. The PSO was widely used
in a variety of practical problems as well as in neural network training
\citep{nnpso1,nnpso2}. Beyond neural network training, Particle Swarm
Optimization has been applied across a broad spectrum of scientific
and technological domains. In particular, it has found significant
use in economic and resource optimization\citep{psoapp1}, as well
as in healthcare management and medical physics, where it supports
efficient medical service allocation and the management of stochastic
patient queue systems\citep{psoapp2}. At the same time, PSO has been
increasingly adopted in physics and chemical engineering for the optimization
of catalytic processes and energy systems, often in combination with
deep learning approaches to extract meaningful physical and chemical
insights\citep{psoapp3}. In more dynamic and decentralized settings,
PSO has proven especially effective in robotics, where it is used
to coordinate swarms of autonomous robots during search-and-rescue
missions operating under uncertain and rapidly changing conditions\citep{psoapp4}.
Finally, its flexibility has also led to successful applications in
bioinformatics and computational biology, where advanced multi-objective
PSO variants are employed for community detection and the identification
of disease-related functional modules within complex biological networks\citep{psoapp5}.\textbf{
}The main steps of the PSO method are outlined in Figure \ref{fig:psoSteps}.
\begin{figure}[H]
\includegraphics[scale=0.65]{pso_schema}

\caption{The main steps of the PSO algorithm.\label{fig:psoSteps}}
\end{figure}


\subsection{The SVM method\label{subsec:The-SVM-method}}

The Support Vector Machine (SVM) is a supervised learning algorithm
applied to both classification and regression problems \citep{svm}.
The concept of Support Vector methods was introduced by V. Vapnik
in 1965, in the context of addressing challenges in pattern recognition.
During the 1990s, V. Vapnik formally introduced Support Vector Machine
(SVM) methods within the framework of Statistical Learning. Since
their introduction, SVMs have been extensively employed across diverse
domains, including pattern recognition, natural language processing,
and related applications \citep{Wang 2022}. For instance, the SVM
algorithm has been employed both in studies on earthquake prediction
and in research on the early detection of seismic phenomena, as demonstrated
in the following works: Hybrid Technique Using Singular Value Decomposition
(SVD) and Support Vector Machine (SVM) Approach for Earthquake Prediction
\citep{Astuti 2014}, Using Support Vector Machine (SVM) with GPS
Ionospheric TEC Estimations to Potentially Predict Earthquake Events
\citep{Asaly 2022}, The efficacy of support vector machines (SVM)
in robust determination of earthquake early warning magnitudes in
central Japan \citep{Reddy 2013}. As with other comparative analyses
of models, SVMs require greater computational resources during training
and exhibit reduced susceptibility to overfitting, whereas neural
networks are generally regarded as more adaptable and capable of scaling
effectively. Beyond these applications, SVMs have also been widely
adopted in several applied scientific and engineering fields. In bioinformatics,
SVMs are commonly employed to analyze and interpret complex biological
data, where their ability to handle high-dimensional feature spaces
is particularly valuable\citep{svmapp1}. In medical imaging and clinical
applications, they have been used effectively for disease classification
and staging, such as the prediction of lung cancer stages from medical
imaging data\citep{svmapp2}. In addition, SVM-based models are increasingly
integrated into healthcare monitoring systems, including Internet
of Medical Things (IoMT) frameworks, where reliability and robustness
are critical requirements\citep{svmapp3}. Moreover, SVMs have been
applied in applied physics and engineering, including the prediction
of electronic properties in advanced material structures and nanocomposites\citep{svmapp4}.
Their robustness has also made them suitable for safety-critical applications,
such as fault detection and diagnosis in nuclear power plant systems\citep{svmapp5}. 

\subsection{The neural network construction method \label{subsec:The-neural-network}}

Another method used in the experiments to predict the category of
seismic vibrations is the method of constructing artificial neural
networks using Grammatical Evolution \citep{nnc}. This method can
detect the optimal architecture for neural networks as well as the
optimal set of values for the corresponding parameters.\textbf{ }This
method was used in various cases, such as chemistry problems \citep{nnc_amide1},
solution of differential equations \citep{nnc_de}, medical problems
\citep{nnc_feas}, educational problems \citep{nnc_student}, detection
of autism \citep{nnc_autism} etc. This method can produce neural
networks in the following form:

\begin{equation}
N\left(\overrightarrow{x},\overrightarrow{w}\right)=\sum_{i=1}^{H}w_{(d+2)i-(d+1)}\sigma\left(\sum_{j=1}^{d}x_{j}w_{(d+2)i-(d+1)+j}+w_{(d+2)i}\right)\label{eq:nn}
\end{equation}
In this equation the value $H$ represents the number of used computation
units (weights) for the neural network. The vector $w$ denotes the
vector containing the parameters of the neural network and vector
$x$ represents the input vector (pattern). The function $\sigma(x)$
stands for the sigmoid function, which is defined as:
\begin{equation}
\sigma(x)=\frac{1}{1+\exp(-x)}
\end{equation}


\subsection{The proposed method\label{subsec:The-proposed-method}}

The method proposed here to tackle the classification of seismic events
is a procedure that constructs artificial features from the original
ones using Grammatical Evolution. The method was initially presented
in the paper of Gavrilis et al \citep{mainfc} and used in various
cases in the past \citep{fc1,fc2,fc3}. The main steps of this method
have as follows:
\begin{enumerate}
\item \textbf{Initialization} step. \textbf{}

\begin{enumerate}
\item \textbf{Obtain }the train data and denote them using the $M$ pairs\textbf{
$\left(x_{i},t_{i}\right),\ i=1..M$}. The values\textbf{ }$t_{i}$
represent the actual output for the input pattern $x_{i}$.
\item \textbf{Set} the parameters of the used genetic algorithm:\textbf{$N_{g}$}
for as the number of allowed generations, \textbf{$N_{c}$ }for the
number of chromosomes,\textbf{ $p_{s}$} for the selection rate and
\textbf{$p_{m}$ }for the\textbf{ }mutation rate.
\item \textbf{Set} as $N_{f}$ the number of artificial features that will
construct the current method.
\item \textbf{Initialize} every chromosome $g_{i},i=1,\ldots,N_{g}$ as
a set of randomly selected integers.
\item \textbf{Set} $k=1$, the generation counter.
\end{enumerate}
\item \textbf{Genetic step}

\begin{enumerate}
\item \textbf{For $i=1,\ldots,N_{g}$ do}

\begin{enumerate}
\item \textbf{Construct} with Grammatical Evolution a set of $N_{f}$ artificial
features for the each chromosome $g_{i}$. The BNF grammar used for
this procedure is shown in Figure \ref{fig:BNF-grammar-of}.
\item \textbf{Transform }the original set of features using the constructed
features and denote the new training set as $\left(x_{g_{i},j},t_{j}\right),\ j=1,..M$
\item \textbf{Train }a machine learning $C$ on the new training set. The
training error for this model will represent the fitness $f_{i}$
for the current chromosome and it is computed as:
\begin{equation}
f_{i}=\sum_{j=1}^{M}\left(C\left(x_{g_{i},j}\right)-t_{j}\right)^{2}
\end{equation}
In the current work the Radial Basis Function networks (RBF) \citep{rbf1,rbf2}
were use d as the machine learning models. A decisive factor for this
choice was the short training time required for these machine learning
models.
\item \textbf{Perform} the selection procedure: Initially the the chromosomes
are sorted according to the associated fitness values and the best\textbf{
$\left(1-p_{s}\right)\times N_{C}$} of them are copied intact to
the next generation.\textbf{ }The remaining chromosomes will be replaced
by offsprings that will be produced during the crossover and the mutation
procedure.
\item \textbf{Perform} the crossover procedure: The outcome of this process
is a set of $p_{s}\times N_{c}$ new chromosomes. For every pair $\left(\tilde{z},\tilde{w}\right)$
of new chromosomes, two distinct chromosomes $z$ and $w$ are chosen
from the current population using the process of tournament selection.
Afterwards, the nee chromosomes are produced using the procedure of
one - point crossover, that is illustrated in in Figure \ref{fig:One-point-crossover}. 
\item \textbf{Execute} the mutation procedure: during this procedure a random
number $r\in\left[0,1\right]$ is selected for each element of every
chromosome. The corresponding element is altered randomly if $r\le p_{m}$. 
\end{enumerate}
\item \textbf{EndFor}
\end{enumerate}
\item \textbf{Set} $k=k+1$
\item \textbf{If} $k\le N_{g}$ goto \textbf{Genetic} Step
\item \textbf{Obtain the best chromosome }$g^{*}$ from the current population.
\item \textbf{Create} the corresponding $N_{f}$ features for $g^{*}$ and
apply this features to the set set and report the corresponding test
error.

\end{enumerate}
Also, a flowchart that presents the overall process of feature construction
is presented in Figure \ref{fig:fcSteps}.
\begin{figure}
\caption{The grammar used for the proposed method.\label{fig:BNF-grammar-of}}

\begin{lyxcode}
S::=\textless expr\textgreater ~~~(0)~

\textless expr\textgreater ~::=~~(\textless expr\textgreater ~\textless op\textgreater ~\textless expr\textgreater )~~(0)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~\textless func\textgreater ~(~\textless expr\textgreater ~)~~~~(1)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar\textless terminal\textgreater ~~~~~~~~~~~~(2)~

\textless op\textgreater ~::=~~~~~+~~~~~~(0)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~-~~~~~~(1)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~{*}~~~~~~(2)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~/~~~~~~(3)

\textless func\textgreater ~::=~~~sin~~(0)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~cos~~(1)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar exp~~~(2)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar log~~~(3)

\textless terminal\textgreater ::=\textless xlist\textgreater ~~~~~~~~~~~~~~~~(0)~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~\textbar\textless digitlist\textgreater .\textless digitlist\textgreater ~(1)

\textless xlist\textgreater ::=x1~~~~(0)~~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~x2~(1)~~~~~~~~~~~~~~

~~~~~~~~~~~………~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~xN~(N)

\textless digitlist\textgreater ::=\textless digit\textgreater ~~~~~~~~~~~~~~~~~~(0)~~~~~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~\textless digit\textgreater\textless digit\textgreater ~~~~~~~~~~~~(1)

~~~~~~~~~~~\textbar ~\textless digit\textgreater\textless digit\textgreater\textless digit\textgreater ~~~~~(2)

\textless digit\textgreater ~~::=~0~(0)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~1~(1)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~2~(2)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~3~(3)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~4~(4)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~5~(5)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~6~(6)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~7~(7)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~8~(8)~~~~~~~~~~~~~

~~~~~~~~~~~\textbar ~9~(9)
\end{lyxcode}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{onepoint_crossover}
\par\end{centering}
\caption{An example of the one - point crossover method.\label{fig:One-point-crossover}}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{fc_flow}
\par\end{centering}
\caption{The steps of the feature construction procedure.\label{fig:fcSteps}}

\end{figure}
 Moreover, a graphical pipeline that summarizes the whole procedure
is depicted in Figure \ref{fig:The-pipeline-of}.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{pipeline}
\par\end{centering}
\caption{The pipeline of the used procedure.\label{fig:The-pipeline-of}}

\end{figure}


\section{Results\label{sec:Results}}

The methods used in the conducted experiments were coded in ANSI C++,
with the assistance of the freely available optimization package of
OPTIMUS \citep{optimus}. Moreover, for the feature construction procedure
the freely available programming tool QFc, that can be downloaded
from \url{https://github.com/itsoulos/QFc}(accessed on 12 December
2025)\citep{qfc} has been used.\textbf{ }Each experiment was conducted
30 times, using different seed for the random generator in each execution.
Also, the procedure of ten - fold cross validation was incorporated
to validate the conducted experiments. The values for the experimental
parameters are listed in Table \ref{tab:The-values-of}. The parameters
in this table have been chosen to provide a compromise between speed
and efficiency of the proposed procedure and were applied to all methods
used in the experiments, so that there is fairness in the comparison
of experimental results. The proposed method of feature construction
was applied to each individual fold and averages of the classification
errors in the control sets were obtained. In addition, the RBF machine
learning model was used to construct the new features, due to the
significantly shorter learning time required for it compared to other
models, such as artificial neural networks.

\begin{table}[H]
\caption{The values of the experimental parameters.\label{tab:The-values-of}}

\centering{}%
\begin{tabular}{ccc}
\hline 
PARAMETER & MEANING & VALUE\tabularnewline
\hline 
$N_{g}$ & Number of generations & 500\tabularnewline
$N_{c}$ & Number of chromosomes & 500\tabularnewline
$p_{s}$ & Selection rate & 0.9\tabularnewline
$p_{m}$ & Mutation rate & 0.05\tabularnewline
$N_{f}$ & Number of features & 2\tabularnewline
\hline 
\end{tabular}
\end{table}
\begin{table}[H]
\caption{Experimental results using a series of machine learning methods.\label{tab:results}}

\centering{}%
\begin{tabular}{cccccc}
DATASET & MLP(GEN) & MLP(PSO) & SVM & NNC & FC\tabularnewline
\hline 
GCT1990 & 39.98\% & 38.21\% & 14.30\% & 22.72\% & 13.50\%\tabularnewline
GCT1995 & 38.08\% & 33.22\% & 13.48\% & 21.50\% & 10.82\%\tabularnewline
GCT2000 & 26.70\% & 27.48\% & 10.19\% & 24.59\% & 6.52\%\tabularnewline
GCT2005 & 38.44\% & 33.19\% & 6.85\% & 27.85\% & 4.45\%\tabularnewline
GCT2010 & 52.89\% & 44.77\% & 14.46\% & 29.81\% & 9.97\%\tabularnewline
\textbf{AVERAGE} & \textbf{39.22\%} & \textbf{35.37\%} & \textbf{11.86\%} & \textbf{25.29\%} & \textbf{9.05\%}\tabularnewline
\hline 
\end{tabular}
\end{table}

Table \ref{tab:results} reports the classification error rates for
five temporal subsets of the seismic dataset (GCT1990, GCT1995, GCT2000,
GCT2005, GCT2010), where the first column encodes the year of the
data and the remaining columns correspond to the machine learning
models MLP(GEN), MLP(PSO), SVM, NNC, and the proposed FC (Future Constructions)
model. The values are expressed as percentage classification error,
that is, the proportion of misclassified events between the two seismic
classes defined during preprocessing (events with magnitude code 2-3
versus events with magnitude greater than 3). The following notation
is used in this table:
\begin{enumerate}
\item The column MLP(GEN) denotes the error from the application of the
genetic algorithm described in subsection \ref{subsec:Global-optimization-methods}
for the training of a neural network with 10 processing nodes.
\item The column MLP(PSO) represents the incorporation of the PSO method
given in subsection \ref{subsec:Global-optimization-methods} for
the training of a neural network with 10 processing nodes.
\item The column SVM represent the application of the SVM method, described
in subsection \ref{subsec:The-SVM-method}. In the current implementation
the freely available library LibSvm \citep{libsvm} was used.
\item The column NNC represents the neural network construction method,
described in subsection \ref{subsec:The-neural-network}.
\item The column FC denotes the proposed feature construction technique,
outlined in subsection \ref{subsec:The-proposed-method}.
\item The row AVERAGE denotes the average classification error for all datasets.
\end{enumerate}
Also, Table \ref{tab:Precision-and-recall} indicates the precision
and recall values for SVM, NNC and the proposed method.
\begin{table}[H]
\caption{Precision and recall values for the methods SVM,NNC and FC.\label{tab:Precision-and-recall}}

\centering{}%
\begin{tabular}{ccccccc}
\hline 
 & \multicolumn{2}{c}{SVM} & \multicolumn{2}{c}{NNC} & \multicolumn{2}{c}{FC}\tabularnewline
\hline 
DATASET & PRECISION & RECALL & PRECISION & RECALL & PRECISION & RECALL\tabularnewline
GCT1990 & 0.798 & 0.837 & 0.684 & 0.728 & 0.800 & 0.874\tabularnewline
GCT1995 & 0.811 & 0.830 & 0.684 & 0.729 & 0.801 & 0.886\tabularnewline
GCT2000 & 0.835 & 0.844 & 0.662 & 0.700 & 0.835 & 0.901\tabularnewline
GCT2005 & 0.856 & 0.869 & 0.628 & 0.627 & 0.856 & 0.904\tabularnewline
GCT2010 & 0.802 & 0.824 & 0.633 & 0.628 & 0.804 & 0.805\tabularnewline
\hline 
\end{tabular}
\end{table}
 

The most striking observation is that FC achieves the lowest error
for all five datasets, with no exceptions. For every GCT year, the
FC column attains the minimum error among all models, demonstrating
a consistent and temporally robust superiority. This pattern is clearly
reflected in the AVERAGE row: the mean classification error of FC
is 9.05\%, whereas the corresponding mean error of the best conventional
baseline, the SVM, is 11.86\%. Moving from 11.86\% to 9.05\% corresponds
to an error reduction of approximately 24-25\%, meaning that roughly
one quarter of the misclassifications made by the SVM are eliminated
when using FC. Compared with the neural approaches, the gain is even
more pronounced: FC reduces the mean error from 39.22\% to 9.05\%
for MLP(GEN), from 35.37\% to 9.05\% for MLP(PSO), and from 25.29\%
to 9.05\% for NNC, effectively absorbing about 64-77\% of their errors. 

Examining the behaviour per year reveals how the proposed model interacts
with the specific characteristics of each temporal subset. Dataset
GCT2005 appears to be the “easiest” case for all models, with the
SVM dropping to 6.85\% error and FC reaching 4.45\%, which corresponds
to an accuracy of about 95.5\%. At the opposite end of the spectrum,
GCT2010 is clearly the most challenging dataset, as indicated by substantially
increased errors for the conventional models (MLP(GEN) 52.89\%, MLP(PSO)
44.77\%, NNC 29.81\%) and by a higher error for the SVM (14.46\%).
Even in this difficult scenario, FC keeps the error below 10\% (9.97\%),
preserving a clear qualitative advantage in the most demanding temporal
setting. A similar pattern emerges for the intermediate years 1995
and 2000, where FC-SVM absolute differences are in the range of 2-4
percentage points, corresponding to roughly 20-35\% relative error
reduction. This suggests that as the data become more complex, the
benefit of the automatically constructed features generated by Grammatical
Evolution becomes increasingly pronounced.

A second important finding is that the ranking of the conventional
models remains stable across all years. The SVM is consistently the
best among the standard baselines, followed by NNC and the two MLP
variants, with MLP(PSO) systematically outperforming MLP(GEN). This
stability reinforces the credibility of the table as a benchmark,
indicating that the results are not driven by noise or random fluctuations
but instead reflect a coherent hierarchy of model performance. On
top of this stable baseline, FC does not merely swap the winner for
a single dataset, it establishes a new performance level by consistently
pushing the error rates to substantially lower values in every year.

From a practical standpoint, given that the final processed dataset
contains on the order of 10,000 seismic events, an average error of
11.86\% compared with 9.05\% implies hundreds fewer misclassified
instances when FC is used instead of a plain SVM trained on the original
features. This has direct implications for early warning and risk
assessment applications, where each reduction in misclassification
translates into more reliable decision-making. The evidence from Table
\ref{tab:results}, combined with the methodological description,
strongly supports the view that the feature construction phase based
on Grammatical Evolution is not a minor refinement over existing classifiers,
but rather a key component that reshapes the feature space so that
the seismic classes become much more separable. This explains why
the proposed FC model achieves an average error of approximately 9\%,
in full agreement with the reported overall accuracy of 91\% in the
abstract. 

Overall, Table 6 shows that FC is not only the best-performing model
in each individual year but also the most stable across different
temporal subsets, confirming that the future-construction approach
with Grammatical Evolution yields a substantial and consistent improvement
over standard machine learning models for the discrimination of seismic
events.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{stat_tbl6}
\par\end{centering}
\caption{Comparison of FC and baseline classifiers on GCT datasets (paired
one-sided t-tests on classification error)\label{fig:stat_tbl6}}
\end{figure}

The repeated-measures ANOVA with Method as the within-subject factor
and DATASET (year) as the blocking factor confirms that the choice
of classifier has a statistically significant effect on the classification
error, indicating that the models are not equivalent in terms of predictive
performance. Building on this global result, we conducted pairwise
paired t-tests between the proposed FC model and each baseline, using
one-sided alternatives that explicitly test the hypothesis that FC
achieves lower mean error than its competitors. The resulting p-values,
visualised as significance stars on the boxplot, show that the superiority
of FC is statistically supported across all comparisons (Figure \ref{fig:stat_tbl6}).

In particular, the comparisons FC vs MLP(GEN) and FC vs NNC yield
{*}{*} (p \textless{} 0.01), indicating that the probability of observing
differences of this magnitude in favour of FC purely by chance is
below 1\%. An even stronger effect emerges for FC vs MLP(PSO), which
is marked with {*}{*}{*} (p \textless{} 0.001), reflecting the very
large performance gap between FC and the PSO-trained MLP. Finally,
the comparison FC vs SVM is annotated with {*} (p \textless{} 0.05),
showing that, although the numerical difference in error is smaller
than in the neural baselines, it remains statistically significant
in favour of FC. Overall, the pattern of significance codes ({*},
{*}{*}, {*}{*}{*}) corroborates the message of Table 6: FC is not
only the best-performing model in terms of average classification
error, but its advantage over all other methods is statistically significant,
with particularly strong evidence against the MLP-based models and
clear, though more moderate, evidence against the already very competitive
SVM baseline.

Also, a graphical summary that represents graphically the comparison
between the different machine learning methods is depicted in Figure
\ref{fig:Graphical-comparison-of}.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{summary}
\par\end{centering}
\caption{Graphical comparison of the used machine learning methods.\label{fig:Graphical-comparison-of}}

\end{figure}

Moreover, the proposed method can identify the hidden relationships
between the features of the objective dataset. As an example of constructed
features consider the following two features created for a distinct
run on the GCT2010 dataset:
\begin{eqnarray*}
f_{1}(x) & = & \sin\left((28.841/9.28)*x_{10}+(-78.60)*x_{3}+16.12*x_{7}+6.3*x_{11}\right)\\
f_{2}(x) & = & \cos\left(458.6*x_{2}+(-18.809)*x_{4}\right)
\end{eqnarray*}

The following figures provide a concise visual characterization of
the artificial features generated by Grammatical Evolution: (i) expression-tree
representations for two representative constructed features ($f_{1}$,$f_{2}$)
and (ii) a complementary summary of the grammar primitives used in
these constructions, offering direct insight into both their structural
form and their basic building blocks.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{figure_comment1_tree_f1}
\par\end{centering}
\caption{Constructed feature $f_{1}(x)$ expression tree\label{fig:cf_f1}}

\end{figure}

Figure \ref{fig:cf_f1} (expression tree of $f_{1}$) visualizes the
internal structure of the first constructed feature as an expression
tree. The root node corresponds to the non-linear primitive sin, applied
to an algebraic core that combines multiple original variables ($x_{10}$,
$x_{3}$, $x_{7}$, $x_{1}$) through a weighted sum with constants.
This diagram provides an interpretable view of the GE output, indicating
that non-linearity is applied over algebraic forms of controlled complexity
assembled from grammar primitives.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{figure_comment1_tree_f2}
\par\end{centering}
\caption{Constructed feature $f_{2}(x)$ expression tree\label{fig:cf_f2}}
\end{figure}

Figure \ref{fig:cf_f2} (expression tree of $f_{2}$) depicts a second
constructed feature with a simpler topology: a cos root applied to
the sum of two variable terms ($x_{2}$, $x_{4}$) with constant coefficients.
Compared to $f_{1}$, this construction has lower structural complexity
(fewer nodes/terms) while still introducing non-linearity. The contrast
between the two trees illustrates that GE can produce features of
varying complexity, ranging from compact to richer expressions, depending
on the optimization objective.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{figure_comment1_primitive_usage}
\par\end{centering}
\caption{Grammar primitives used in example constructed features ($f_{1}$,
$f_{2}$)\label{fig:Grammarf1f2}}
\end{figure}

Figure \ref{fig:Grammarf1f2} (primitive usage) summarizes the grammar
building blocks used in the example constructions ($f_{1}$,$f_{2}$),
including non-linear functions (sin/cos), arithmetic operators (+,
{*}, /), and terminal variables $x_{i}$. Bar heights correspond to
simple occurrence counts (node counts) of primitives within the two
symbolic expressions, while numeric constants are omitted to keep
the visualization compact and focused on structural elements. This
plot provides an intuitive view of the ``vocabulary” employed in
these constructions, and the same analysis can be extended by aggregating
counts over all runs to describe broader construction trends.

\subsection{Experiments with the number of features}

An additional experiment was conducted, where the number of constructed
features was changed from 1 to 4 for the proposed feature construction
method. The corresponding experimental results are outlined in Table
\ref{tab:resultsFC}.
\begin{table}[H]
\caption{Experimental results using the proposed method and a series of values
for the number of constructed features $N_{f}$.\label{tab:resultsFC}}

\centering{}%
\begin{tabular}{ccccc}
\hline 
DATASET & $N_{f}=1$ & $N_{f}=2$ & $N_{f}=3$ & $N_{f}=4$\tabularnewline
\hline 
GCT1990 & 11.03\% & 13.50\% & 14.62\% & 15.40\%\tabularnewline
GCT1995 & 9.41\% & 10.82\% & 9.98\% & 10.05\%\tabularnewline
GCT2000 & 6.51\% & 6.52\% & 6.75\% & 6.54\%\tabularnewline
GCT2005 & 4.40\% & 4.45\% & 4.42\% & 4.38\%\tabularnewline
GCT2010 & 9.72\% & 9.97\% & 9.82\% & 10.32\%\tabularnewline
\textbf{AVERAGE} & \textbf{8.21\%} & \textbf{9.05\%} & \textbf{9.12\%} & \textbf{9.34\%}\tabularnewline
\hline 
\end{tabular}
\end{table}

Table \ref{tab:resultsFC} investigates the behavior of the proposed
FC model as the number of constructed features N\_f varies from 1
to 4. The results show that performance is generally very stable,
with the mean classification error ranging within a narrow band between
8.21\% (for $N_{f}$ = 1) and 9.34\% (for $N_{f}$ = 4). The lowest
average error is obtained with a single constructed feature ($N_{f}$
=1), whereas the configuration $N_{f}$ =2, which is adopted as the
default setting in Table \ref{tab:results}, yields a mean error of
9.05\%, very close to the optimum and with highly consistent behaviour
across all years. In some datasets, such as GCT2005, slightly better
values appear for larger $N_{f}$ (e.g., 4.38\% for $N_{f}$ = 4 versus
4.40\% for $N_{f}$ = 1), but these gains are marginal and do not
change the overall picture. Taken together, the results indicate that
FC does not require a large number of future constructions to perform
well: one or two constructed features are sufficient to achieve very
low error rates, while further increasing $N_{f}$ does not lead to
systematic improvements and likely introduces redundant information
that does not translate into better generalization. This supports
the view that the quality of the features generated by Grammatical
Evolution is more important than their quantity, and that the proposed
choice $N_{f}$ = 2 offers a well-balanced compromise between performance
and model simplicity. Also, Figure \ref{fig:nfexpers}presents a comparison
for the test error for various cases of the parameter $N_{f}$.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{nfexper}
\par\end{centering}
\caption{The performance of the proposed method on the used datasets, using
various values for the parameter $N_{f}$.\label{fig:nfexpers}}

\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{stat_tbl7}
\par\end{centering}
\caption{Comparison of FC and baseline classifiers on GCT datasets (paired
one-sided t-tests on classification error)\label{fig:stat_tbl7}}
\end{figure}

The boxplot for Table \ref{tab:resultsFC}, together with the paired
t-tests, shows that none of the comparisons among the $N_{f}$ = 1,
$N_{f}$ = 2, $N_{f}$ = 3 and $N_{f}$ = 4 configurations reaches
statistical significance (all labelled as ns). This indicates that,
given the available datasets, the performance of the FC model is essentially
insensitive to the number of future constructions, and that using
smaller values such as $N_{f}$ = 1 or $N_{f}$ = 2 achieves comparable
accuracy without any statistically supported gains from increasing
$N_{f}$ (Figure \ref{fig:stat_tbl7}).

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{Nf1}
\par\end{centering}
\caption{Effect of the number of constructed features $N_{f}$: consistency
of $N_{f}$=1 and diminishing returns\label{fig:Nf1}}
\end{figure}

The Figure \ref{fig:Nf1} summarizes how the classification error
varies with the number of constructed features $N_{f}$. The left
panel plots error versus $N_{f}$ for each dataset, together with
the mean trend (\textpm{} standard error), indicating that the lowest
average error is obtained at $N_{f}$=1. The right panel reports the
per-dataset difference $\Delta$ = Error ($N_{f}$=k) - Error ($N_{f}$=
1) for k\ensuremath{\ge}2, where the dashed line $\Delta$=0 corresponds
to the $N_{f}$=1 baseline. Predominantly positive $\Delta$ values
show that increasing $N_{f}$ provides no additional benefit and leads
to diminishing returns relative to $N_{f}$=1.

\subsection{Experiments with the number of generations}

Moreover, in order to test the efficiency of the proposed method,
an additional experiment was executed, where the number of generations
was altered from 50 to 400.
\begin{table}[H]
\caption{Experimental results using the proposed method and a series of values
for the number generations $N_{g}$.\label{tab:resultsNG}}

\centering{}%
\begin{tabular}{ccccc}
\hline 
DATASET & $N_{g}=50$ & $N_{g}=100$ & $N_{g}=200$ & $N_{g}=400$\tabularnewline
\hline 
GCT1990 & 13.61\% & 13.19\% & 13.50\% & 12.30\%\tabularnewline
GCT1995 & 11.06\% & 11.70\% & 10.82\% & 10.74\%\tabularnewline
GCT2000 & 6.53\% & 6.58\% & 6.52\% & 6.52\%\tabularnewline
GCT2005 & 4.58\% & 4.45\% & 4.45\% & 4.41\%\tabularnewline
GCT2010 & 9.88\% & 10.00\% & 9.97\% & 9.94\%\tabularnewline
\textbf{AVERAGE} & \textbf{9.13\%} & \textbf{9.18\%} & \textbf{9.05\%} & \textbf{8.78\%}\tabularnewline
\hline 
\end{tabular}
\end{table}

Table \ref{tab:resultsNG} focuses on the number of generations $N_{g}$
of the evolutionary algorithm during feature construction and evaluates
four values (50, 100, 200, 400). The results demonstrate that FC is
remarkably robust with respect to this hyperparameter: the mean classification
error remains very close across settings, ranging from 9.18\% for
$N_{g}$ = 100 down to 8.78\% for $N_{g}$ = 400. The configuration
$N_{g}$ = 200, which is used as the default in the previous analysis,
attains an average error of 9.05\%, essentially indistinguishable
from the more expensive setting $N_{g}$ = 400, which improves the
mean error only by about a quarter of a percentage point. At the level
of individual datasets there are cases where a larger number of generations
yields more noticeable gains (for instance, GCT1990 improves to 12.30\%
at $N_{g}$ = 400), but the overall pattern is that most of the benefit
is already captured within 100-200 generations, with further iterations
providing diminishing returns. Thus, Table \ref{tab:resultsNG} suggests
that the Grammatical Evolution process converges to useful future
constructions relatively quickly, and that the choice $N_{g}$ = 200
offers a very good trade-off between computational cost and predictive
performance, avoiding unnecessary increases in training time without
delivering substantial accuracy gains. Furthermore, Figure \ref{fig:ngexpers}
outlines the performance of the proposed method ti the used datasets,
using different values for the number of generations $N_{g}.$

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{ngexper}
\par\end{centering}
\caption{The performance of the proposed method using different values for
the parameter $N_{g}$.\label{fig:ngexpers}}

\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{stat_tbl8}
\par\end{centering}
\caption{Comparison of FC and baseline classifiers on GCT datasets (paired
one-sided t-tests on classification error)\label{fig:stat_tbl8}}
\end{figure}

In Figure \ref{fig:stat_tbl8}, the paired t-tests for the different
numbers of generations show that all pairwise comparisons are non-significant
(ns), indicating that variations in $N_{g}$ do not materially affect
the classification error of the FC model.

\section{Conclusions\label{sec:Conclusions}}

This study investigates the use of Grammatical Evolution for constructing
artificial features in earthquake prediction, applying several machine
learning approaches, including MLP(GEN), MLP(PSO), SVM, and NNC, alongside
Feature Construction (FC). The analysis is based on seismic data recorded
between 1990 and 2015 within the geographical area defined by latitudes
33°-44° and longitudes 17°-44°. While all the aforementioned methods
belong to the domain of machine learning, FC is distinguished as a
feature engineering technique. Specifically, Feature Construction
(FC) refers to the process of generating new, informative attributes
from the existing dataset, thereby enhancing the representational
capacity of the data and improving the performance of machine learning
models. Following these steps, our experiments demonstrated that the
FC technique yielded the best results, achieving the lowest mean error
9.05\% corresponding to an overall accuracy of 91\%. The SVM method
achieved the second-best performance, with an average error of 11.86\%.
Consequently, we proceeded with the FC technique, which yielded the
best results, and implemented artificially constructed features $N_{f}$
with ranging from 1 to 4. Furthermore, to evaluate the efficiency
of the proposed method, an additional experiment was conducted in
which the number of generations $N_{g}$ was varied from 50 to 400.
Consequently, we applied 1, 2, 3, and 4 constructed features within
this technique, with the single constructed feature$N_{f}$ =1 exhibiting
superior performance compared to the others producing the minimal
average error 8.21\%. Our experiment was further extended by incorporating
a series of values for the number of generations$N_{g}$ (50, 100,
200, and 400), and the results indicated that $N_{g}$ 400 generations
yielded the best performance 8.78\%. In contrast, selecting $N_{g}$
200 provides also an effective balance between computational cost
and predictive performance. This study was conducted as a direct response
to the challenge identified in our previous research, thereby extending
and refining the scope of the earlier findings.

Specifically, the study demonstrates the potential of Grammatical
Evolution as a novel feature engineering tool in seismology, offering
a new perspective on how artificial features can enhance earthquake
prediction models. At the same time, certain limitations must be acknowledged,
such as the imbalance in the dataset and the restriction to a specific
geographical region, which may affect generalizability. Future research
should therefore explore the application of this approach to diverse
seismic catalogs, investigate its integration with machines learning
architectures. By articulating these implications, limitations, and
directions, the contribution of the study becomes clearer and more
impactful.

\vspace{6pt}


\authorcontributions{Conceptualization, C.K. and I.G.T.; methodology, C.K.; software,
I.G.T.; validation, G.K. C.S., V.C.; formal analysis, G.K..; investigation,
C.K.; resources, C.S.; data curation, C.K..; writing original draft
preparation, C.K.; writing review and editing, I.G.T.; visualization,
V.C.; supervision, C.S.; project administration, C.S.; funding acquisition,
C.S. All authors have read and agreed to the published version of
the manuscript.}

\funding{This research has been financed by the European Union : Next Generation
EU through the Program Greece 2.0 National Recovery and Resilience
Plan , under the call RESEARCH-CREATE-INNOVATE, project name “iCREW:
Intelligent small craft simulator for advanced crew training using
Virtual Reality techniques\textquotedbl{} (project code:TAEDK-06195).}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable.}

\dataavailability{The original contributions presented in this study are included in
the article. Further inquiries can be directed to the corresponding
author.}

\conflictsofinterest{The authors declare no conflicts of interest.}

\begin{adjustwidth}{-\extralength}{0cm}{}


\reftitle{References}
\begin{thebibliography}{999}
\bibitem[(2017)]{UNDRR 2017} United Nations Office for Disaster Risk
Reduction (UNDRR). (2017). Sendai framework terminology on disaster
risk reduction. Resilience.

\bibitem{Nakamura 1984} Nakamura, Y. (1984). Development of earthquake
early-warning system for the Shinkansen, some recent earthquake engineering
research and practice in Japan. Proceeding of The Japanese National
Committee of the International Association for Earthquake Engineering,
June 1984, 224--238.

\bibitem{Nakamura 1988} Nakamura, Y. (1988). On the urgent earthquake
detection and alarm system (UrEDAS). In Proc. 9th World Conference
on Earthquake Engineering, 1988 (pp. 673-678).

\bibitem{Espinosa Aranda 1995} Espinosa Aranda, J. M., Jimenez, A.,
Ibarrola, G., Alcantar, F., Aguilar, A., Inostroza, M., \& Maldonado,
S. (1995). Mexico City seismic alert system. Seismological Research
Letters, 66, 42-53

\bibitem{Kamigaichi 2009} Kamigaichi, O., Saito, M., Doi, K., Matsumori,
T., Tsukada, S. Y., Takeda, K., ... \& Watanabe, Y. (2009). Earthquake
early warning in Japan: Warning the general public and future prospects.
Seismological Research Letters, 80(5), 717-726.

\bibitem{EEW} Earthquake Early Warning System (Japan). Wikipedia.
Retrieved November 14, 2025, from \url{https://en.wikipedia.org/wiki/Earthquake_Early_Warning_(Japan)} 

\bibitem{Wenzel 1999} Wenzel, F., Oncescu, M. C., Baur, M., Fiedrich,
F., \& Ionescu, C. (1999). An early warning system for Bucharest.
Seismological Research Letters, 70(2), 161-169. 

\bibitem{Alcik 2009} Alcik, H., Ozel, O., Apaydin, N., \& Erdik,
M. (2009). A study on warning algorithms for Istanbul earthquake early
warning system. Geophysical Research Letters, 36(5). 

\bibitem{Satriano 2009} Satriano, C., Elia, L., Martino, C., Lancieri,
M., Zollo, A., \& Iannaccone, G. (2009, December). The Earthquake
Early Warning System for Southern Italy: Concepts, Capabilities and
Future Perspectives. In AGU Fall Meeting Abstracts (Vol. 2009, pp.
S13A-1726). 

\bibitem{USGS} ShakeAlert. US Geological Survey, Earthquake Hazards
Program. Retrieved November 15, 2025, from \url{https://earthquake.usgs.gov/data/shakealert/} 

\bibitem{Shake alert}ShakeAlert. Earthquake Early Warning (EEW) System.
Retrieved November 15, 2025, from \url{https://www.shakealert.org/ }

\bibitem[(2012)]{zollo}Zollo, A., Festa, G., Emolo, A., \& Colombelli,
S. (2015). Source characterization for earthquake early warning. In
Encyclopedia of Earthquake Engineering (pp. 3327-3346). Springer,
Berlin, Heidelberg.

\bibitem{Tsoulos 2025} Kopitsa, C., Tsoulos, I. G., \& Charilogis,
V. (2025). Predicting the Magnitude of Earthquakes Using Grammatical
Evolution. Algorithms, 18(7), 405. 

\bibitem{Earthquake app} Earthquake app. Retrieved November 16, 2025,
from \url{https://earthquake.app/#banner_1} 

\bibitem{Android EAS} Android Earthquake Alert System. Retrieved
November 16, 2025, from \url{https://crisisresponse.google/android-early-earthquake-warnings }

\bibitem{Greece Earthquakes} Greece Earthquakes. Retrieved November
16, 2025, from \url{https://play.google.com/store/apps/details?id=com.greek.Earthquake&pli=1} 

\bibitem{Earthquake Network} Earthquake Network. Retrieved November
16, 2025, from\url{ https://sismo.app/ }. 

\bibitem{Housner 1964} Housner, G. W., \& Jennings, P. C. (1964).
Generation of artificial earthquakes. Journal of the Engineering Mechanics
Division, 90(1), 113-150. 

\bibitem{Adeli 2009} Adeli, H., \& Panakkat, A. (2009). A probabilistic
neural network for earthquake magnitude prediction. Neural networks,
22(7), 1018-1024.\textbf{ }

\bibitem{Zhou 2012} Zhou, Q., Tong, G., Xie, D., Li, B., \& Yuan,
X. (2012). A seismic-based feature extraction algorithm for robust
ground target classification. IEEE Signal Processing Letters, 19(10),
639-642. 

\bibitem{Mart=0000EDnez-=0000C1lvarez 2013} Martínez-Álvarez, F.,
Reyes, J., Morales-Esteban, A., \& Rubio-Escudero, C. (2013). Determining
the best set of seismicity indicators to predict earthquakes. Two
case studies: Chile and the Iberian Peninsula. Knowledge-Based Systems,
50, 198-210. 

\bibitem{Schmidt 2015} Schmidt, L., Hegde, C., Indyk, P., Lu, L.,
Chi, X., \& Hohl, D. (2015, April). Seismic feature extraction using
Steiner tree methods. In 2015 IEEE international conference on acoustics,
speech and signal processing (ICASSP) (pp. 1647-1651). IEEE. 

\bibitem{Narayanakumar 2016} Narayanakumar, S., \& Raja, K. (2016).
A BP artificial neural network model for earthquake magnitude prediction
in Himalayas, India. Circuits and Systems, 7(11), 3456-3468. 

\bibitem{Cortes 2016} Asencio-Cortés, G., Martínez-Álvarez, F., Morales-Esteban,
A., \& Reyes, J. (2016). A sensitivity study of seismicity indicators
in supervised learning to improve earthquake prediction. Knowledge-Based
Systems, 101, 15-30. 

\bibitem{Asim 2018} Asim, K. M., Idris, A., Iqbal, T., \& Martínez-Álvarez,
F. (2018). Earthquake prediction model using support vector regressor
and hybrid neural networks. PloS one, 13(7), e0199004. 

\bibitem{Chamberlain 2018} Chamberlain, C. J., \& Townend, J. (2018).
Detecting real earthquakes using artificial earthquakes: On the use
of synthetic waveforms in matched‐filter earthquake detection. Geophysical
Research Letters, 45(21), 11-641. 

\bibitem{Okada 2018} Okada, A., \& Kaneda, Y. (2018, May). Neural
network learning: Crustal state estimation method from time-series
data. In 2018 International Conference on Control, Artificial Intelligence,
Robotics \& Optimization (ICCAIRO) (pp. 141-146). IEEE. 

\bibitem{Lin 2018} Lin, J. W., Chao, C. T., \& Chiou, J. S. (2018).
Determining neuronal number in each hidden layer using earthquake
catalogues as training data in training an embedded back propagation
neural network for predicting earthquake magnitude. Ieee Access, 6,
52582-52597. 

\bibitem{Zhang 2019} Zhang, L., Si, L., Yang, H., Hu, Y., \& Qiu,
J. (2019). Precursory pattern based feature extraction techniques
for earthquake prediction. IEEE Access, 7, 30991-31001. 

\bibitem{Rojas 2019} Rojas, O., Otero, B., Alvarado, L., Mus, S.,
\& Tous, R. (2019). Artificial neural networks as emerging tools for
earthquake detection. Computación y Sistemas, 23(2), 335-350. 

\bibitem{Ali 2020} Ali, A., Sheng-Chang, C., \& Shah, M. (2020).
Continuous wavelet transformation of seismic data for feature extraction.
SN Applied Sciences, 2(11), 1835. 

\bibitem{Bamer 2021} Bamer, F., Thaler, D., Stoffel, M., \& Markert,
B. (2021). A monte carlo simulation approach in non-linear structural
dynamics using convolutional neural networks. Frontiers in Built Environment,
7, 679488. 

\bibitem{Wang 2023} Wang, T., Bian, Y., Zhang, Y., \& Hou, X. (2023).
Using artificial intelligence methods to classify different seismic
events. Seismological Society of America, 94(1), 1-16. 

\bibitem{Ozkaya 2024} Ozkaya, S. G., Baygin, M., Barua, P. D., Tuncer,
T., Dogan, S., Chakraborty, S., \& Acharya, U. R. (2024). An automated
earthquake classification model based on a new butterfly pattern using
seismic signals. Expert Systems with Applications, 238, 122079. 

\bibitem{Sinha 2025} Sinha, D. K., \& Kulkarni, S. (2025, June).
Advancing Seismic Prediction through Machine Learning: A Comprehensive
Review of the Transformative Impact of Feature Engineering. In 2025
International Conference on Emerging Trends in Industry 4.0 Technologies
(ICETI4T) (pp. 1-8). IEEE. 

\bibitem{Mahmoud 2025} Mahmoud, A., Alrusaini, O., Shafie, E., Aboalndr,
A., \& Elbelkasy, M. S. (2025). Machine Learning-Based Earthquake
Prediction: Feature Engineering and Model Performance Using Synthetic
Seismic Data. Appl. Math, 19(3), 695-702. 

\bibitem[(2002)]{mainGe}O'Neill, M., \& Ryan, C. (2002). Grammatical
evolution. IEEE Transactions on Evolutionary Computation, 5(4), 349-358. 

\bibitem[(2017)]{gaOverview}Kramer, O. (2017). Genetic algorithms.
In Genetic algorithm essentials (pp. 11-19). Cham: Springer International
Publishing. 

\bibitem[(2002)]{bnf1}Backus, J. W. (1959). The syntax and the semantics
of the proposed international algebraic language of the Zurich ACM-GAMM
Conference. In ICIP Proceedings (pp. 125-132).

\bibitem{ge_program1}Ryan, C., Collins, J. J., \& Neill, M. O. (1998,
April). Grammatical evolution: Evolving programs for an arbitrary
language. In European conference on genetic programming (pp. 83-96).
Berlin, Heidelberg: Springer Berlin Heidelberg.

\bibitem{ge_program2}O’Neill, M., \& Ryan, C. (1999, May). Evolving
multi-line compilable C programs. In European Conference on Genetic
Programming (pp. 83-92). Berlin, Heidelberg: Springer Berlin Heidelberg.

\bibitem{ge_credit}Brabazon, A., \& O'Neill, M. (2006). Credit classification
using grammatical evolution. Informatica, 30(3).

\bibitem{ge_intrusion}Şen, S., \& Clark, J. A. (2009, March). A grammatical
evolution approach to intrusion detection on mobile ad hoc networks.
In Proceedings of the second ACM conference on Wireless network security
(pp. 95-102).

\bibitem{ge_water}Chen, L., Tan, C. H., Kao, S. J., \& Wang, T. S.
(2008). Improvement of remote monitoring on water quality in a subtropical
reservoir by incorporating grammatical evolution with parallel genetic
algorithms into satellite imagery. Water Research, 42(1-2), 296-306.

\bibitem{ge_glykemia}Hidalgo, J. I., Colmenar, J. M., Risco-Martin,
J. L., Cuesta-Infante, A., Maqueda, E., Botella, M., \& Rubio, J.
A. (2014). Modeling glycemia in humans by means of grammatical evolution.
Applied Soft Computing, 20, 40-53.

\bibitem{ge_ant}Tavares, J., \& Pereira, F. B. (2012, April). Automatic
design of ant algorithms with grammatical evolution. In European conference
on genetic programming (pp. 206-217). Berlin, Heidelberg: Springer
Berlin Heidelberg.

\bibitem{ge_datacenter}Zapater, M., Risco-Martín, J. L., Arroba,
P., Ayala, J. L., Moya, J. M., \& Hermida, R. (2016). Runtime data
center temperature prediction using grammatical evolution techniques.
Applied Soft Computing, 49, 94-107.

\bibitem{ge_trig}Ryan, C., O’Neill, M., \& Collins, J. J. (1998,
June). Grammatical evolution: Solving trigonometric identities. In
proceedings of Mendel (Vol. 98, p. 4th). Brno, Czech Republic: Technical
University of Brno, Faculty of Mechanical Engineering.

\bibitem{ge_music}de la Puente, A. O., Alfonso, R. S., \& Moreno,
M. A. (2002, June). Automatic composition of music by means of grammatical
evolution. In Proceedings of the 2002 conference on APL: array processing
languages: lore, problems, and applications (pp. 148-155).

\bibitem{ge_nn}De Campos, L. M. L., de Oliveira, R. C. L., \& Roisenberg,
M. (2016). Optimization of neural networks through grammatical evolution
and a genetic algorithm. Expert Systems with Applications, 56, 368-384.

\bibitem{ge_nn2}Soltanian, K., Ebnenasir, A., \& Afsharchi, M. (2022).
Modular grammatical evolution for the generation of artificial neural
networks. Evolutionary computation, 30(2), 291-327.

\bibitem{ge_constant}Dempsey, I., O'Neill, M., \& Brabazon, A. (2007).
Constant creation in grammatical evolution. International Journal
of Innovative Computing and Applications, 1(1), 23-38.

\bibitem{ge_pacman}Galván-López, E., Swafford, J. M., O’Neill, M.,
\& Brabazon, A. (2010, April). Evolving a ms. pacman controller using
grammatical evolution. In European Conference on the Applications
of Evolutionary Computation (pp. 161-170). Berlin, Heidelberg: Springer
Berlin Heidelberg.

\bibitem{ge_supermario}Shaker, N., Nicolau, M., Yannakakis, G. N.,
Togelius, J., \& O'neill, M. (2012, September). Evolving levels for
super mario bros using grammatical evolution. In 2012 IEEE Conference
on Computational Intelligence and Games (CIG) (pp. 304-311). IEEE.

\bibitem{ge_energy}Martínez‐Rodríguez, D., Colmenar, J. M., Hidalgo,
J. I., Villanueva Micó, R. J., \& Salcedo‐Sanz, S. (2020). Particle
swarm grammatical evolution for energy demand estimation. Energy Science
\& Engineering, 8(4), 1068-1079.

\bibitem{ge_comb}Sabar, N. R., Ayob, M., Kendall, G., \& Qu, R. (2013).
Grammatical evolution hyper-heuristic for combinatorial optimization
problems. IEEE Transactions on Evolutionary Computation, 17(6), 840-861.

\bibitem{ge_crypt}Ryan, C., Kshirsagar, M., Vaidya, G., Cunningham,
A., \& Sivaraman, R. (2022). Design of a cryptographically secure
pseudo random number generator with grammatical evolution. Scientific
reports, 12(1), 8602.

\bibitem{ge_decision}Pereira, P. J., Cortez, P., \& Mendes, R. (2021).
Multi-objective grammatical evolution of decision trees for mobile
marketing user conversion prediction. Expert Systems with Applications,
168, 114287.

\bibitem{ge_analog}Castejón, F., \& Carmona, E. J. (2018). Automatic
design of analog electronic circuits using grammatical evolution.
Applied Soft Computing, 62, 1003-1018.

\bibitem{Tsoulos 2023} Tsoulos, I. G., Tzallas, A., \& Karvounis,
E. (2023). Constructing the Bounds for Neural Network Training Using
Grammatical Evolution. Computers, 12(11), 226. 

\bibitem{Tsoulos 2024} Tsoulos, I. G., Varvaras, I., \& Charilogis,
V. (2024). RbfCon: Construct Radial Basis Function Neural Networks
with Grammatical Evolution. Software (2674-113X), 3(4). 

\bibitem[(2022)]{Wang}Wang, X., Zhong, Z., Yao, Y., Li, Z., Zhou,
S., Jiang, C., \& Jia, K. (2023). Small Earthquakes Can Help Predict
Large Earthquakes: A Machine Learning Perspective. Applied Sciences,
13(11), 6424.

\bibitem[(2012)]{gutenberg}Gutenberg, B., \& Richter, C. F. (1955).
Magnitude and energy of earthquakes. Nature, 176(4486), 795-795.

\bibitem[(2022)]{zhu_rapid}Zhu, J., Zhou, Y., Liu, H., Jiao, C.,
Li, S., Fan, T., ... \& Song, J. (2023). Rapid earthquake magnitude
classification using single station data based on the machine learning.
IEEE Geoscience and Remote Sensing Letters, 21, 1-5.

\bibitem[(2018)]{nn1}Abiodun, O. I., Jantan, A., Omolara, A. E.,
Dada, K. V., Mohamed, N. A., \& Arshad, H. (2018). State-of-the-art
in artificial neural network applications: A survey. Heliyon, 4(11). 

\bibitem{nn2}Suryadevara, S., \& Yanamala, A. K. Y. (2021). A Comprehensive
Overview of Artificial Neural Networks: Evolution, Architectures,
and Applications. Revista de Inteligencia Artificial en Medicina,
12(1), 51-76. 

\bibitem[(2018)]{gen1}Sivanandam, S. N., \& Deepa, S. N. (2008).
Genetic algorithms. In Introduction to genetic algorithms (pp. 15-37).
Berlin, Heidelberg: Springer Berlin Heidelberg. 

\bibitem[(2018)]{nnga1}Kalogirou, S. A. (2004). Optimization of solar
systems using artificial neural-networks and genetic algorithms. Applied
Energy, 77(4), 383-405. 

\bibitem[(2018)]{nnga2}Chiroma, H., Noor, A. S. M., Abdulkareem,
S., Abubakar, A. I., Hermawan, A., Qin, H., ... \& Herawan, T. (2017).
Neural networks optimization through genetic algorithm searches: a
review. Appl. Math. Inf. Sci, 11(6), 1543-1564. 

\bibitem[(2012)]{genapp1}Manning, T., Sleator, R. D., \& Walsh, P.
(2013). Naturally selecting solutions: the use of genetic algorithms
in bioinformatics. Bioengineered, 4(5), 266-278.

\bibitem[(2012)]{genapp2}Maulik, U. (2009). Medical image segmentation
using genetic algorithms. IEEE Transactions on information technology
in biomedicine, 13(2), 166-173.

\bibitem[(2012)]{genapp3}Ghaheri, A., Shoar, S., Naderan, M., \&
Hoseini, S. S. (2015). The applications of genetic algorithms in medicine.
Oman medical journal, 30(6), 406.

\bibitem[(2012)]{genapp4}Mousavi-Avval, S. H., Rafiee, S., Sharifi,
M., Hosseinpour, S., Notarnicola, B., Tassielli, G., \& Renzulli,
P. A. (2017). Application of multi-objective genetic algorithms for
optimization of energy, economics and environmental life cycle assessment
in oilseed production. Journal of Cleaner Production, 140, 804-815.

\bibitem[(2012)]{genapp5}Zhang, W., Xie, Y., He, H., Long, Z., Zhuang,
L., \& Zhou, J. (2025). Multi-physics coupling model parameter identification
of lithium-ion battery based on data driven method and genetic algorithm.
Energy, 314, 134120.

\bibitem[(2018)]{gen2}Kramer, O. (2017). Genetic algorithms. In Genetic
algorithm essentials (pp. 11-19). Cham: Springer International Publishing. 

\bibitem[(2018)]{pso1}Wang, D., Tan, D., \& Liu, L. (2018). Particle
swarm optimization algorithm: an overview. Soft computing, 22(2),
387-408. 

\bibitem[(2018)]{pso2}Jain, N. K., Nangia, U., \& Jain, J. (2018).
A review of particle swarm optimization. Journal of The Institution
of Engineers (India): Series B, 99(4), 407-411. 

\bibitem[(2018)]{nnpso1}Meissner, M., Schmuker, M., \& Schneider,
G. (2006). Optimized Particle Swarm Optimization (OPSO) and its application
to artificial neural network training. BMC bioinformatics, 7(1), 125. 

\bibitem[(2018)]{nnpso2}Garro, B. A., \& Vázquez, R. A. (2015). Designing
artificial neural networks using particle swarm optimization algorithms.
Computational intelligence and neuroscience, 2015(1), 369298. 

\bibitem[(2012)]{psoapp1}Li, S. (2025). Economic optimization of
business administration resources: Multi-objective scheduling method
based on improved PSO. Journal of Computational Methods in Sciences
and Engineering, 14727978251337955.

\bibitem[(2012)]{psoapp2}Wang, C. H., Tian, R., Hu, K., Chen, Y.
T., \& Ku, T. H. (2025). A Markov decision optimization of medical
service resources for two-class patient queues in emergency departments
via particle swarm optimization algorithm. Scientific Reports, 15(1),
2942.

\bibitem[(2012)]{psoapp3}Bao, R., Wang, Z., Guo, Q., Wu, X., \& Yang,
Q. (2025). Bio-Digital Catalyst Design: Generative Deep Learning for
Multi-Objective Optimization and Chemical Insights in CO2 Methanation.
ACS Catalysis, 15(15), 12691-12714.

\bibitem[(2012)]{psoapp4}Kumar, T. R., Nandhini, T. J., Jumaniyazova,
I., Abdulla, H., Jumaniyozov, Y., \& Bhatt, V. (2025, May). Swarm
Robotics for Search and Rescue Operations in Disaster Zones Using
Particle Swarm Optimization (PSO) Algorithms. In 2025 International
Conference on Networks and Cryptology (NETCRYPT) (pp. 870-875). IEEE.

\bibitem[(2012)]{psoapp5}Zhu, X., Bi, M., Shang, J., Sun, Y., Li,
F., Zhang, Y., ... \& Liu, J. X. (2025). MPSO-CD: a Multi-objective
Particle Swarm Optimization Community Detection Method for Identifying
Disease Modules. IEEE Transactions on Computational Biology and Bioinformatics

\bibitem[(2016)]{svm}Suthaharan, S. (2016). Support vector machine.
In Machine learning models and algorithms for big data classification:
thinking with examples for effective learning (pp. 207-235). Boston,
MA: Springer US. 

\bibitem{Wang 2022} Wang, Q. (2022, June). Support vector machine
algorithm in machine learning. In 2022 IEEE international conference
on artificial intelligence and computer applications (ICAICA) (pp.
750-756). IEEE.

\bibitem{Astuti 2014} Astuti, W., Akmeliawati, R., Sediono, W., \&
Salami, M. J. E. (2014). Hybrid technique using singular value decomposition
(SVD) and support vector machine (SVM) approach for earthquake prediction.
IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing, 7(5), 1719-1728. 

\bibitem{Asaly 2022} Asaly, S., Gottlieb, L. A., Inbar, N., \& Reuveni,
Y. (2022). Using support vector machine (SVM) with GPS ionospheric
TEC estimations to potentially predict earthquake events. Remote Sensing,
14(12), 2822. 

\bibitem{Reddy 2013} Reddy, R., \& Nair, R. R. (2013). The efficacy
of support vector machines (SVM) in robust determination of earthquake
early warning magnitudes in central Japan. Journal of Earth System
Science, 122(5), 1423-1434. 

\bibitem[(2012)]{svmapp1}Mahata, K., Sengupta, S., Biswas, M., Ghosh,
S., Banerjee, A. K., Pati, S. K., \& Mal, C. (2025). Application of
Machine Learning in Bioinformatics: Capture and Interpret Biological
Data. In Machine Learning in Biomedical and Health Informatics (pp.
239-262). Apple Academic Press.

\bibitem[(2012)]{svmapp2}Manimaran, P., Vignesh, R., Vignesh, B.,
\& Thilak, G. (2025, February). Enhanced Prediction of Lung Cancer
Stages using SVM and Medical Imaging. In 2025 International Conference
on Electronics and Renewable Systems (ICEARS) (pp. 1334-1338). IEEE.

\bibitem[(2012)]{svmapp3}Kazi, K. S. L. (2025). Machine learning-driven
internet of medical things (ML-IoMT)-based healthcare monitoring system.
In Responsible AI for digital health and medical analytics (pp. 49-86).
IGI Global Scientific Publishing.

\bibitem[(2012)]{svmapp4}Azizian‐Kalandaragh, Y., Barkhordari, A.,
\& Badali, Y. (2025). Support vector machine for prediction of the
electronic factors of a Schottky configuration interlaid with pure
PVC and doped by Sm2O3 nanoparticles. Advanced Electronic Materials,
11(6), 2400624.

\bibitem[(2012)]{svmapp5}Sun, Y., Song, M., Song, C., Zhao, M., \&
Yang, Y. (2025). KPCA-based fault detection and diagnosis model for
the chemical and volume control system in nuclear power plants. Annals
of Nuclear Energy, 211, 110973.

\bibitem{nnc}Tsoulos, I., Gavrilis, D., \& Glavas, E. (2008). Neural
network construction and training using grammatical evolution. Neurocomputing,
72(1-3), 269-277.

\bibitem{nnc_amide1}Papamokos, G. V., Tsoulos, I. G., Demetropoulos,
I. N., \& Glavas, E. (2009). Location of amide I mode of vibration
in computed data utilizing constructed neural networks. Expert Systems
with Applications, 36(10), 12210-12213.

\bibitem{nnc_de}Tsoulos, I. G., Gavrilis, D., \& Glavas, E. (2009).
Solving differential equations with constructed neural networks. Neurocomputing,
72(10-12), 2385-2391.

\bibitem{nnc_feas}Tsoulos, I. G., Mitsi, G., Stavrakoudis, A., \&
Papapetropoulos, S. (2019). Application of machine learning in a Parkinson's
disease digital biomarker dataset using neural network construction
(NNC) methodology discriminates patient motor status. Frontiers in
ICT, 6, 10.

\bibitem{nnc_student}Christou, V., Tsoulos, I., Loupas, V., Tzallas,
A. T., Gogos, C., Karvelis, P. S., ... \& Giannakeas, N. (2023). Performance
and early drop prediction for higher education students using machine
learning. Expert Systems with Applications, 225, 120079.

\bibitem{nnc_autism}Toki, E. I., Pange, J., Tatsis, G., Plachouras,
K., \& Tsoulos, I. G. (2024). Utilizing Constructed Neural Networks
for Autism Screening. Applied Sciences, 14(7), 3053.

\bibitem[(2018)]{mainfc}Gavrilis, D., Tsoulos, I. G., \& Dermatas,
E. (2008). Selecting and constructing features using grammatical evolution.
Pattern Recognition Letters, 29(9), 1358-1365.

\bibitem[(2018)]{fc1}Georgoulas, G., Gavrilis, D., Tsoulos, I. G.,
Stylios, C., Bernardes, J., \& Groumpos, P. P. (2007). Novel approach
for fetal heart rate classification introducing grammatical evolution.
Biomedical Signal Processing and Control, 2(2), 69-79.

\bibitem[(2018)]{fc2}Smart, O., Tsoulos, I. G., Gavrilis, D., \&
Georgoulas, G. (2011). Grammatical evolution for features of epileptic
oscillations in clinical intracranial electroencephalograms. Expert
systems with applications, 38(8), 9991-9999.

\bibitem[(2018)]{fc3}Tzallas, A. T., Tsoulos, I., Tsipouras, M. G.,
Giannakeas, N., Androulidakis, I., \& Zaitseva, E. (2016, November).
Classification of EEG signals using feature creation produced by grammatical
evolution. In 2016 24th Telecommunications Forum (TELFOR) (pp. 1-4).
IEEE.

\bibitem{rbf1}Park, J., \& Sandberg, I. W. (1991). Universal approximation
using radial-basis-function networks. Neural computation, 3(2), 246-257.

\bibitem{rbf2}Yu, H., Xie, T., Paszczynski, S., \& Wilamowski, B.
M. (2011). Advantages of radial basis function networks for dynamic
system design. IEEE Transactions on Industrial Electronics, 58(12),
5438-5450.

\bibitem[(2018)]{optimus}Tsoulos, I. G., Charilogis, V., Kyrou, G.,
Stavrou, V. N., \& Tzallas, A. (2025). OPTIMUS: A Multidimensional
Global Optimization Package. Journal of Open Source Software, 10(108),
7584.

\bibitem[(2022)]{qfc}Tsoulos, I. G. (2022). QFC: A Parallel Software
Tool for Feature Construction, Based on Grammatical Evolution. Algorithms,
15(8), 295.

\bibitem[(2018)]{libsvm}Chang, C. C., \& Lin, C. J. (2011). LIBSVM:
A library for support vector machines. ACM transactions on intelligent
systems and technology (TIST), 2(3), 1-27. 

\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors' response\\
%Reviewer 2 comments and authors' response\\
%Reviewer 3 comments and authors' response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\PublishersNote{}

\end{adjustwidth}{}
\end{document}
